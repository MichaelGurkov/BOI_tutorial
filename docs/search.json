[
  {
    "objectID": "tree_regression/tree_regression_exercise_set.html",
    "href": "tree_regression/tree_regression_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "tree_regression/tree_regression_exercise_set.html#task-1-load-necessary-libraries",
    "href": "tree_regression/tree_regression_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "tree_regression/tree_regression_exercise_set.html#task-2-load-the-data",
    "href": "tree_regression/tree_regression_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "tree_regression/tree_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "tree_regression/tree_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "tree_regression/tree_regression_exercise_set.html#task-4-make-predictions",
    "href": "tree_regression/tree_regression_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "tree_classification/tree_classification_exercise_set.html",
    "href": "tree_classification/tree_classification_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "tree_classification/tree_classification_exercise_set.html#task-1-load-necessary-libraries",
    "href": "tree_classification/tree_classification_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "tree_classification/tree_classification_exercise_set.html#task-2-load-the-data",
    "href": "tree_classification/tree_classification_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "tree_classification/tree_classification_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "tree_classification/tree_classification_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "tree_classification/tree_classification_exercise_set.html#task-4-make-predictions",
    "href": "tree_classification/tree_classification_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "simple_regression/simple_regression_exercise_set.html",
    "href": "simple_regression/simple_regression_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "simple_regression/simple_regression_exercise_set.html#task-1-load-necessary-libraries",
    "href": "simple_regression/simple_regression_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "simple_regression/simple_regression_exercise_set.html#task-2-load-the-data",
    "href": "simple_regression/simple_regression_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "simple_regression/simple_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "simple_regression/simple_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "simple_regression/simple_regression_exercise_set.html#task-4-make-predictions",
    "href": "simple_regression/simple_regression_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "multiple_regression/multiple_regression.html",
    "href": "multiple_regression/multiple_regression.html",
    "title": "Multiple Linear Regression with Boston Housing Data",
    "section": "",
    "text": "Introduction\nIn this tutorial, we’ll explore how to perform a multiple linear regression using the Boston housing dataset. Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and multiple independent variables. Here, we’ll investigate how the median value of owner-occupied homes (medv) is influenced by the average number of rooms per dwelling (rm) and the proportion of owner-occupied units built before 1940 (age).\nWe’ll be using the tidyverse and tidymodels libraries in R for data manipulation and modeling. Let’s get started!\n\n\nLoad Necessary Libraries\nFirst, we’ll load the required libraries. The tidyverse package provides tools for data manipulation and visualization, while tidymodels is a suite of packages for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\nLoad the Data\nNext, we’ll load the Boston housing dataset and select the relevant columns.\n\nboston_data = MASS::Boston %&gt;%\n  select(rm, age, medv)\n\n\n\nFit the Multiple Linear Regression Model\nNow, we’ll fit a multiple linear regression model. Our goal is to predict medv using rm and age. We will create a preprocessing recipe to handle interactions.\nThe recipe package, part of the tidymodels ecosystem, provides a way to preprocess data before modeling. We define the transformations to apply to our data in a consistent and reusable manner using “steps”.\n\nstep_interact(): Creates interaction terms between variables.\n\n\npreprocess_recipe = recipe(medv ~ ., data = boston_data) %&gt;% \n  step_interact(terms = ~rm:age)\n\nWe then define our model specification using linear_reg().\n\nmodel_spec = linear_reg()\n\nNext, we use a workflow, a concept from the tidymodels package that helps streamline the modeling process by bundling together the preprocessing and modeling steps. This makes the workflow easier to manage and ensures that all steps are consistently applied.\n\nadd_recipe(): Adds the preprocessing recipe to the workflow.\nadd_model(): Adds the model specification to the workflow.\n\nFinally, we fit the workflow to the data.\n\nlinear_model_workflow = workflow() %&gt;% \n  add_recipe(preprocess_recipe) %&gt;% \n  add_model(model_spec) %&gt;% \n  fit(boston_data)\n\nAlternatively, we can specify a model without an intercept.\n\npreprocess_recipe = recipe(medv ~ ., data = boston_data) %&gt;% \n  step_interact(terms = ~rm:age)\n\nmodel_spec = linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nlinear_model_workflow = workflow() %&gt;% \n  add_recipe(preprocess_recipe) %&gt;% \n  add_model(model_spec, formula = medv ~ 0 + . -rm) %&gt;% \n  fit(boston_data)\n\n\n\nInspect the Model Coefficients\nAfter fitting the model, it’s essential to inspect the coefficients to understand the relationship between the variables.\n\nlinear_model_workflow %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -59.6       7.79       -7.65 1.01e-13\n2 rm           13.7       1.20       11.4  4.39e-27\n3 age           0.382     0.0967      3.94 9.15e- 5\n4 rm_x_age     -0.0714    0.0151     -4.72 3.03e- 6\n\n\n\n\nMake Predictions\nFinally, we’ll use our model to make predictions.\n\npredictions = linear_model_workflow %&gt;% \n  predict(boston_data)\n\n\n\nConclusion\nIn this tutorial, we demonstrated how to perform a multiple linear regression using the Boston housing dataset. We went through loading the data, fitting a model with preprocessing steps, inspecting the coefficients, and making predictions. This technique can be extended to include more variables and interactions, providing a robust tool for understanding complex relationships in your data.\nFeel free to explore further by adding more variables or trying different types of models. Happy coding!"
  },
  {
    "objectID": "knn_regression/knn_regression.html",
    "href": "knn_regression/knn_regression.html",
    "title": "KNN Regression with Boston Housing Data",
    "section": "",
    "text": "Introduction\nIn this tutorial, we’ll explore how to perform K-Nearest Neighbors (KNN) regression using the Boston housing dataset. KNN regression is a non-parametric method that makes predictions based on the k-nearest neighbors to a data point. This method is particularly useful when the relationship between variables is complex and nonlinear. Here, we’ll investigate how the median value of owner-occupied homes (medv) is influenced by the average number of rooms per dwelling (rm) using KNN regression.\nWe’ll be using the tidyverse and tidymodels libraries in R for data manipulation and modeling. Let’s get started!\n\n\nLoad Necessary Libraries\nFirst, we’ll load the required libraries. The tidyverse package provides tools for data manipulation and visualization, while tidymodels is a suite of packages for modeling. We will also load the MASS package which contains the Boston dataset.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\nLoad the Data\nNext, we’ll select the relevant columns from the Boston housing dataset. We’ll ensure there are no duplicate rows.\n\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\nFit the KNN Regression Model\nNow, we’ll fit a K-Nearest Neighbors (KNN) regression model. Our goal is to predict medv using rm. We set the number of neighbors to 100.\nThe nearest_neighbor() function from parsnip is used to specify the KNN model. Here, we set the mode to “regression” and specify the number of neighbors. We then fit the model to our data.\n\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n  ) %&gt;%\n  fit(medv ~ rm, data = boston_data)\n\n\n\nMake Predictions\nFinally, we’ll use our model to make predictions.\n\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)\n\n\n\nConclusion\nIn this tutorial, we demonstrated how to perform KNN regression using the Boston housing dataset. We went through loading the data, fitting a KNN model, and making predictions. KNN regression is a versatile technique that can capture complex relationships in data without assuming a specific functional form.\nFeel free to explore further by adjusting the number of neighbors or trying different predictors. This method can be a powerful tool in your machine learning toolkit. Happy coding!"
  },
  {
    "objectID": "knn_classification/knn_classification.html",
    "href": "knn_classification/knn_classification.html",
    "title": "KNN Classification with Default Data",
    "section": "",
    "text": "Introduction\nIn this tutorial, we’ll explore how to perform K-Nearest Neighbors (KNN) classification using the Default dataset. KNN classification is a non-parametric method that classifies data points based on the k-nearest neighbors to a data point. This method is particularly useful for classification tasks when the relationship between features and the target variable is complex and nonlinear. Here, we’ll investigate how to classify default status based on balance using KNN classification.\nWe’ll be using the tidyverse and tidymodels libraries in R for data manipulation and modeling. Let’s get started!\n\n\nLoad Necessary Libraries\nFirst, we’ll load the required libraries. The tidyverse package provides tools for data manipulation and visualization, while tidymodels is a suite of packages for modeling. We will also load the ISLR package which contains the dataset.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR)\n\n\n\nLoad the Data\nNext, we’ll select the relevant columns from the Default dataset. We’ll filter out any rows with missing values in balance, and ensure the default column is treated as a factor.\n\ndefault_data = Default %&gt;% \n  select(balance, default) %&gt;% \n  filter(!is.na(balance)) %&gt;% \n  mutate(default = factor(default))\n\n\n\nFit the KNN Classification Model\nNow, we’ll fit a K-Nearest Neighbors (KNN) classification model. Our goal is to classify default status using balance.\nThe nearest_neighbor() function from parsnip is used to specify the KNN model. Here, we set the mode to “classification”. We then fit the model to our data.\n\nknn_model_fit = nearest_neighbor(mode = \"classification\") %&gt;% \n  fit(default ~ balance, data = default_data)\n\n\n\nMake Predictions\nNext, we’ll use our model to make predictions.\n\npredictions = knn_model_fit %&gt;% \n  predict(default_data)\n\n\n\nEvaluate the Model\nTo evaluate the performance of our KNN classification model, we’ll create a confusion matrix.\nA confusion matrix is a table used to describe the performance of a classification model. It compares the actual values with the values predicted by the model. The confusion matrix provides the following metrics:\n\nTrue Positives (TP): The number of positive class predictions that are actually positive.\nTrue Negatives (TN): The number of negative class predictions that are actually negative.\nFalse Positives (FP): The number of negative class predictions that are actually positive.\nFalse Negatives (FN): The number of positive class predictions that are actually negative.\n\nThese metrics help us understand the accuracy, precision, recall, and overall performance of the classification model.\n\ndefault_data %&gt;% \n  select(default) %&gt;% \n  bind_cols(predictions) %&gt;% \n  conf_mat(truth = default, estimate = .pred_class)\n\n          Truth\nPrediction   No  Yes\n       No  9639  163\n       Yes   28  170\n\n\n\n\nConclusion\nIn this tutorial, we demonstrated how to perform KNN classification using the Default dataset. We went through loading the data, fitting a KNN classification model, making predictions, and evaluating the model using a confusion matrix. KNN classification is a versatile technique that can capture complex relationships in data and is useful for various classification tasks.\nFeel free to explore further by adjusting the number of neighbors or trying different predictors. This method can be a powerful tool in your machine learning toolkit. Happy coding! ```"
  },
  {
    "objectID": "dplyr/dplyr_exercises.html",
    "href": "dplyr/dplyr_exercises.html",
    "title": "Tutorials for DS course materials",
    "section": "",
    "text": "# load libraries\n\nlibrary(tidyverse)\n\nSample Data Frame for Exercises\n\n# Make data for exercises\n\ndata = tibble(\n  id = 1:10,\n  name = c(\n    \"Alice\",\n    \"Bob\",\n    \"Charlie\",\n    \"David\",\n    \"Eva\",\n    \"Frank\",\n    \"Grace\",\n    \"Hannah\",\n    \"Ian\",\n    \"Jack\"\n  ),\n  age = c(23, 45, 34, 27, 19, 31, 29, 41, 36, 24),\n  score = c(85, 92, 88, 91, 76, 83, 77, 89, 94, 78)\n)\n\n\nExercises\n\nSelect the columns name and age.\n\n\n\nShow the code\nselected_data = data %&gt;%\n  select(name, age)\n\nselected_data\n\n\n# A tibble: 10 × 2\n   name      age\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 Alice      23\n 2 Bob        45\n 3 Charlie    34\n 4 David      27\n 5 Eva        19\n 6 Frank      31\n 7 Grace      29\n 8 Hannah     41\n 9 Ian        36\n10 Jack       24\n\n\n\nSelect the columns id, name, and score.\n\n\n\nShow the code\nselected_data = data %&gt;%\n  select(id, name, score)\n\nselected_data\n\n\n# A tibble: 10 × 3\n      id name    score\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     1 Alice      85\n 2     2 Bob        92\n 3     3 Charlie    88\n 4     4 David      91\n 5     5 Eva        76\n 6     6 Frank      83\n 7     7 Grace      77\n 8     8 Hannah     89\n 9     9 Ian        94\n10    10 Jack       78\n\n\n\nFilter the rows where age is greater than 30.\n\n\n\nShow the code\nfiltered_data = data %&gt;%\n  filter(age &gt; 30)\n\nfiltered_data\n\n\n# A tibble: 5 × 4\n     id name      age score\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     2 Bob        45    92\n2     3 Charlie    34    88\n3     6 Frank      31    83\n4     8 Hannah     41    89\n5     9 Ian        36    94\n\n\n\nFilter the rows where score is less than 80.\n\n\n\nShow the code\nfiltered_data = data %&gt;%\n  filter(score &lt; 80)\n\nfiltered_data\n\n\n# A tibble: 3 × 4\n     id name    age score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5 Eva      19    76\n2     7 Grace    29    77\n3    10 Jack     24    78\n\n\n\nCreate a new column age_in_months which is age multiplied by 12.\n\n\n\nShow the code\nmutated_data = data %&gt;%\n  mutate(age_in_months = age * 12)\n\nmutated_data\n\n\n# A tibble: 10 × 5\n      id name      age score age_in_months\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1     1 Alice      23    85           276\n 2     2 Bob        45    92           540\n 3     3 Charlie    34    88           408\n 4     4 David      27    91           324\n 5     5 Eva        19    76           228\n 6     6 Frank      31    83           372\n 7     7 Grace      29    77           348\n 8     8 Hannah     41    89           492\n 9     9 Ian        36    94           432\n10    10 Jack       24    78           288\n\n\n\nCreate a new column score_category which is “high” if score is greater than 90 and “low” otherwise.\n\n\n\nShow the code\nmutated_data = data %&gt;%\n  mutate(score_category = ifelse(score &gt; 90, \"high\", \"low\"))\n\nmutated_data\n\n\n# A tibble: 10 × 5\n      id name      age score score_category\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n 1     1 Alice      23    85 low           \n 2     2 Bob        45    92 high          \n 3     3 Charlie    34    88 low           \n 4     4 David      27    91 high          \n 5     5 Eva        19    76 low           \n 6     6 Frank      31    83 low           \n 7     7 Grace      29    77 low           \n 8     8 Hannah     41    89 low           \n 9     9 Ian        36    94 high          \n10    10 Jack       24    78 low           \n\n\n\nCalculate the average score for the entire dataset.\n\n\n\nShow the code\nsummary_data = data %&gt;%\n  summarize(avg_score = mean(score))\n\nsummary_data\n\n\n# A tibble: 1 × 1\n  avg_score\n      &lt;dbl&gt;\n1      85.3\n\n\n\nCalculate the average age and the maximum score.\n\n\n\nShow the code\nsummary_data = data %&gt;%\n  summarize(avg_age = mean(age), max_score = max(score))\n\nsummary_data\n\n\n# A tibble: 1 × 2\n  avg_age max_score\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    30.9        94\n\n\n\nArrange the dataset by age in ascending order.\n\n\n\nShow the code\narranged_data = data %&gt;%\n  arrange(age)\n\narranged_data\n\n\n# A tibble: 10 × 4\n      id name      age score\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     5 Eva        19    76\n 2     1 Alice      23    85\n 3    10 Jack       24    78\n 4     4 David      27    91\n 5     7 Grace      29    77\n 6     6 Frank      31    83\n 7     3 Charlie    34    88\n 8     9 Ian        36    94\n 9     8 Hannah     41    89\n10     2 Bob        45    92\n\n\n\nArrange the dataset by score in descending order.\n\n\n\nShow the code\narranged_data = data %&gt;%\n  arrange(desc(score))\n\narranged_data\n\n\n# A tibble: 10 × 4\n      id name      age score\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     9 Ian        36    94\n 2     2 Bob        45    92\n 3     4 David      27    91\n 4     8 Hannah     41    89\n 5     3 Charlie    34    88\n 6     1 Alice      23    85\n 7     6 Frank      31    83\n 8    10 Jack       24    78\n 9     7 Grace      29    77\n10     5 Eva        19    76"
  },
  {
    "objectID": "datasets/default.html",
    "href": "datasets/default.html",
    "title": "Introduction to the Default Data Set",
    "section": "",
    "text": "The Default data set, included in the ISLR package, is frequently utilized in classification tasks within machine learning and statistical analysis. This data set comprises information about 10,000 individuals, offering insights into factors that might influence loan default behavior. It includes variables indicating default status, student status, average credit card balance, and income."
  },
  {
    "objectID": "datasets/default.html#variables-in-the-default-data-set",
    "href": "datasets/default.html#variables-in-the-default-data-set",
    "title": "Introduction to the Default Data Set",
    "section": "Variables in the Default Data Set",
    "text": "Variables in the Default Data Set\nHere is an explanation of each variable in the Default data set, along with their names and data types:\n\ndefault: Indicates whether the individual defaulted on their loan (Factor: No, Yes)\nstudent: Indicates whether the individual is a student (Factor: No, Yes)\nbalance: The average balance that the individual has remaining on their credit card (Numeric)\nincome: The individual’s income (Numeric)"
  },
  {
    "objectID": "datasets/default.html#loading-the-default-data-set",
    "href": "datasets/default.html#loading-the-default-data-set",
    "title": "Introduction to the Default Data Set",
    "section": "Loading the Default Data Set",
    "text": "Loading the Default Data Set\nIn R, data sets from specific packages are often loaded using the :: operator, which allows you to access a dataset or function from a particular package without attaching the entire package. This is particularly useful to avoid name conflicts or reduce the clutter in your namespace.\nTo load the Default data set from the ISLR package, you can use the following code:\n\n# Load the Default data set\ndata(\"Default\", package = \"ISLR\")\n\nHere’s a step-by-step explanation of the code:\n\ndata(“Default”, package = “ISLR”): This line loads the Default data set. The data function is a base R function that loads specified data sets. By providing the name of the data set (\"Default\") and the package from which it comes (\"ISLR\"), R understands where to find the data set.\nISLR::Default: Alternatively, you can directly reference the data set using the :: operator as follows:\n\n\nDefault = ISLR::Default\n\nThe :: operator allows you to access the Default data set from the ISLR package directly. This method does not load the entire ISLR package into your R session, but only the specific data set or function you are interested in."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "",
    "text": "Welcome to this tutorial on the Bayes classifier. This tutorial aims to introduce the concept of the Bayes classifier, provide a detailed explanation of the code involved, and demonstrate its implementation using R. We will also compare the Bayes classifier with the K-Nearest Neighbors (KNN) classifier to give you a broader perspective on classification methods."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#what-is-the-bayes-classifier",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#what-is-the-bayes-classifier",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "What is the Bayes Classifier?",
    "text": "What is the Bayes Classifier?\nThe Bayes classifier is a probabilistic model that uses Bayes’ Theorem to predict the class of a given data point based on prior knowledge of conditions related to the class. It is particularly useful for scenarios where the probability distribution of the features is known. The classifier assigns a data point to the class with the highest posterior probability."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#loading-necessary-libraries",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#loading-necessary-libraries",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Loading Necessary Libraries",
    "text": "Loading Necessary Libraries\nFirst, we need to load the required libraries for our analysis. We will use tidyverse for data manipulation and visualization, mvtnorm for generating multivariate normal distributions, and tidymodels for model fitting.\n\nlibrary(tidyverse)\nlibrary(mvtnorm)\nlibrary(tidymodels)"
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#setting-parameters",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#setting-parameters",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Setting Parameters",
    "text": "Setting Parameters\nNext, we define the parameters for our simulation. These include the means of the distributions for two classes (A and B), the covariance matrix, and the number of observations.\n\nparams = list()\nparams$class_a_means = list(c(4, 2), c(4, 8), c(6, 10))\nparams$class_b_means = c(6, 2)\nparams$sigma = diag(2)\nparams$obs_num = 200\n\n\nclass_a_means: List of mean vectors for class A.\nclass_b_means: Mean vector for class B.\nsigma: Covariance matrix (assumed to be the same for both classes).\nobs_num: Number of observations to be generated for each class."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#defining-the-bayes-classifier-function",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#defining-the-bayes-classifier-function",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Defining the Bayes Classifier Function",
    "text": "Defining the Bayes Classifier Function\nWe define a function to classify data points based on the Bayes theorem. This function calculates the probability of a data point belonging to each class and assigns the class with the highest probability.\n\nget_bayes_classification = function(x, y) {\n  obs_value = c(x, y)\n  \n  class_a_prob = map_dbl(params$class_a_means,\n                         ~ dmvnorm(x = obs_value, mean = ., sigma = params$sigma)) %&gt;% \n    mean()\n  \n  class_b_prob = dmvnorm(x = obs_value, mean = params$class_b_means, sigma = params$sigma)\n  \n  class = if_else(class_a_prob &gt;= class_b_prob, \"A\", \"B\")\n  \n  return(class)\n}\n\n\nobs_value: The observation vector.\nclass_a_prob: The average probability of the observation belonging to class A.\nclass_b_prob: The probability of the observation belonging to class B.\nclass: The assigned class based on the higher probability."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#simulating-data-for-two-groups",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#simulating-data-for-two-groups",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Simulating Data for Two Groups",
    "text": "Simulating Data for Two Groups\nWe simulate data points for classes A and B. Class A data points are sampled from a mixture of three different normal distributions, while class B data points are sampled from a single normal distribution.\n\nsampled_means = sample(\n  params$class_a_means,\n  size = params$obs_num,\n  replace = TRUE,\n  prob = rep(1 / length(params$class_a_means), length(params$class_a_means))\n)\n\ntrain_data = map(sampled_means, ~ as.data.frame(rmvnorm(\n  n = 1,\n  mean = .,\n  sigma = params$sigma\n))) %&gt;%\n  list_rbind() %&gt;%\n  mutate(class = \"A\") %&gt;%\n  bind_rows(\n    rmvnorm(n = params$obs_num, params$class_b_means, params$sigma) %&gt;%\n      as.data.frame() %&gt;%\n      mutate(class = \"B\")\n  ) %&gt;%\n  set_names(c(\"x\", \"y\", \"class\"))\n\nrm(sampled_means)\n\ntest_data = tibble(x = seq(min(train_data$x), max(train_data$x), length.out = 50),\n                   y = seq(min(train_data$y), max(train_data$y), length.out = 50)) %&gt;% \n  expand.grid()\n\n\nsampled_means: Randomly sampled mean vectors for class A.\ntrain_data: Combined data frame for classes A and B.\ntest_data: Grid of points for testing the classifier."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#visualizing-the-training-data",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#visualizing-the-training-data",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Visualizing the Training Data",
    "text": "Visualizing the Training Data\nWe visualize the training data to understand the distribution of the two classes.\n\ntrain_data %&gt;% \n  ggplot(aes(x, y, color = class)) + \n  geom_point() + \n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#applying-the-bayes-classifier",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#applying-the-bayes-classifier",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Applying the Bayes Classifier",
    "text": "Applying the Bayes Classifier\nWe apply the Bayes classifier to the test data and visualize the classification results.\n\nbayes_class = test_data %&gt;% \n  mutate(class = map2_chr(x, y, get_bayes_classification))\n\nggplot() + \n  geom_point(data = train_data, aes(x = x, y = y, color = class)) +\n  geom_point(data = bayes_class, aes(x = x, y = y, color = class), alpha = 0.5)"
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#k-nearest-neighbors-knn-classifier",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#k-nearest-neighbors-knn-classifier",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "K-Nearest Neighbors (KNN) Classifier",
    "text": "K-Nearest Neighbors (KNN) Classifier\nTo provide a comparison, we implement the KNN classifier and visualize its results.\n\npreproc = recipe(class ~ x + y, data = train_data) \n\nknn_class = map(c(1, 10, 100), function(temp_k) {\n  knn_spec = nearest_neighbor(neighbors = temp_k,\n                              mode = \"classification\", engine = \"kknn\")\n\n  knn_wf = workflow() %&gt;%\n    add_recipe(preproc) %&gt;%\n    add_model(knn_spec) %&gt;%\n    fit(train_data)\n  \n  pred = knn_wf %&gt;% \n    predict(test_data) %&gt;% \n    rename(!!sym(as.character(temp_k)) := 1)\n  \n  return(pred)\n}) %&gt;% \n  list_cbind()\n\nknn_class = test_data %&gt;% \n  bind_cols(knn_class)\n\nggplot() + \n  geom_point(data = knn_class %&gt;% \n               pivot_longer(-c(x, y),\n                            names_to = \"k\",\n                            values_to = \"class\"),\n             aes(x = x, y = y, color = class), alpha = 0.5) + \n  geom_point(data = train_data, aes(x = x, y = y, color = class)) +\n  facet_wrap(~k)\n\n\n\n\n\n\n\n\nIn this code: - preproc: Preprocessing recipe for the data. - knn_spec: Specification of the KNN model with different values of k. - knn_wf: Workflow for fitting the KNN model. - knn_class: Predictions of the KNN model for the test data."
  },
  {
    "objectID": "bayes_classifier_simulation/bayes_classifier_simulation.html#conclusion",
    "href": "bayes_classifier_simulation/bayes_classifier_simulation.html#conclusion",
    "title": "Introduction to Bayes Classifier with R: A Simulation",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial has provided an in-depth look at the Bayes classifier, from concept to implementation in R. By comparing it with the KNN classifier, we have demonstrated different approaches to classification problems. Understanding these methods and their applications will enhance your ability to analyze and interpret data effectively."
  },
  {
    "objectID": "datasets/boston.html",
    "href": "datasets/boston.html",
    "title": "Introduction to the Boston Data Set",
    "section": "",
    "text": "The Boston data set, famously used in various machine learning and statistical analysis projects, provides a comprehensive look at housing values in the suburbs of Boston. It has been widely used for regression tasks, particularly predicting house prices based on various features. This data set, which originates from the U.S. Census Bureau, contains 506 observations of 14 different variables, each representing different characteristics of the suburbs in Boston."
  },
  {
    "objectID": "datasets/boston.html#variables-in-the-boston-data-set",
    "href": "datasets/boston.html#variables-in-the-boston-data-set",
    "title": "Introduction to the Boston Data Set",
    "section": "Variables in the Boston Data Set",
    "text": "Variables in the Boston Data Set\nHere is an explanation of each variable in the Boston data set, along with their names and data types:\n\ncrim: Per capita crime rate by town (Numeric)\nzn: Proportion of residential land zoned for lots over 25,000 sq. ft. (Numeric)\nindus: Proportion of non-retail business acres per town (Numeric)\nchas: Charles River dummy variable (1 if tract bounds river; 0 otherwise) (Binary)\nnox: Nitrogen oxides concentration (parts per 10 million) (Numeric)\nrm: Average number of rooms per dwelling (Numeric)\nage: Proportion of owner-occupied units built prior to 1940 (Numeric)\ndis: Weighted distances to five Boston employment centers (Numeric)\nrad: Index of accessibility to radial highways (Ordinal)\ntax: Full-value property tax rate per $10,000 (Numeric)\nptratio: Pupil-teacher ratio by town (Numeric)\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town (Numeric)\nlstat: Percentage of lower status of the population (Numeric)\nmedv: Median value of owner-occupied homes in $1000s (Numeric)"
  },
  {
    "objectID": "datasets/boston.html#loading-the-boston-data-set",
    "href": "datasets/boston.html#loading-the-boston-data-set",
    "title": "Introduction to the Boston Data Set",
    "section": "Loading the Boston Data Set",
    "text": "Loading the Boston Data Set\nIn R, data sets from specific packages are often loaded using the :: operator, which allows you to access a dataset or function from a particular package without attaching the entire package. This is particularly useful when you want to avoid name conflicts or simply reduce the clutter in your namespace.\nTo load the Boston data set from the MASS package, you can use the following code:\n\n# Load the Boston data set\ndata(\"Boston\", package = \"MASS\")\n\nHere’s a step-by-step explanation of the code:\n\ndata(“Boston”, package = “MASS”): This line loads the Boston data set. The data function is a base R function that loads specified data sets. By providing the name of the data set (\"Boston\") and the package from which it comes (\"MASS\"), R understands where to find the data set.\nMASS::Boston: Alternatively, you can directly reference the data set using the :: operator as follows:\n\n\nBoston &lt;- MASS::Boston\n\nThe :: operator allows you to access the Boston data set from the MASS package directly. This method does not load the entire MASS package into your R session, but only the specific data set or function you are interested in."
  },
  {
    "objectID": "dplyr/dplyr.html",
    "href": "dplyr/dplyr.html",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "In this tutorial, we’ll explore some of the most useful verbs in the dplyr package for data manipulation: select, filter, mutate, summarize, and arrange. We’ll use the Palmer penguins dataset to demonstrate each of these functions. The palmerpenguins package provides data on three penguin species from the Palmer Archipelago, Antarctica.\n\n\nFirst, we’ll load the required libraries and the Palmer penguins dataset.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Take a look at the data\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nThe select function allows us to choose specific columns from the dataset. For instance, we might only be interested in the species, island, and bill length columns.\n\n# Select specific columns\npenguins_selected = penguins %&gt;%\n  select(species, island, bill_length_mm)\n\n# View the first few rows of the selected columns\nhead(penguins_selected)\n\n# A tibble: 6 × 3\n  species island    bill_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;\n1 Adelie  Torgersen           39.1\n2 Adelie  Torgersen           39.5\n3 Adelie  Torgersen           40.3\n4 Adelie  Torgersen           NA  \n5 Adelie  Torgersen           36.7\n6 Adelie  Torgersen           39.3\n\n\n\n\n\nThe filter function helps us subset rows based on specific conditions. Let’s filter the data to only include observations where the bill length is greater than 40 mm.\n\n# Filter rows where bill length is greater than 40 mm\npenguins_filtered = penguins %&gt;%\n  filter(bill_length_mm &gt; 40)\n\n# View the first few rows of the filtered data\nhead(penguins_filtered)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           40.3          18                 195        3250\n2 Adelie  Torgersen           42            20.2               190        4250\n3 Adelie  Torgersen           41.1          17.6               182        3200\n4 Adelie  Torgersen           42.5          20.7               197        4500\n5 Adelie  Torgersen           46            21.5               194        4200\n6 Adelie  Biscoe              40.6          18.6               183        3550\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\nWith mutate, we can create new columns or transform existing ones. Suppose we want to calculate the bill length in centimeters.\n\n# Create a new column for bill length in cm\npenguins_mutated = penguins %&gt;%\n  mutate(bill_length_cm = bill_length_mm / 10)\n\n# View the first few rows to see the new column\nhead(penguins_mutated)\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_cm &lt;dbl&gt;\n\n\n\n\n\nThe summarize function allows us to compute summary statistics. To get a summary of the average bill length by species, we can use group_by in combination with summarize.\n\n# Summarize average bill length by species\npenguins_summary = penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(avg_bill_length = mean(bill_length_mm, na.rm = TRUE))\n\n# View the summary\npenguins_summary\n\n# A tibble: 3 × 2\n  species   avg_bill_length\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Adelie               38.8\n2 Chinstrap            48.8\n3 Gentoo               47.5\n\n\n\n\n\nFinally, arrange allows us to sort the data. Let’s sort the penguins dataset by bill length in descending order.\n\n# Sort the data by bill length in descending order\npenguins_arranged = penguins %&gt;%\n  arrange(desc(bill_length_mm))\n\n# View the first few rows of the sorted data\nhead(penguins_arranged)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n4 Chinstrap Dream            55.8          19.8               207        4000\n5 Gentoo    Biscoe           55.1          16                 230        5850\n6 Gentoo    Biscoe           54.3          15.7               231        5650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\nWe can combine these verbs to perform more complex data manipulations. For example, let’s select the species, island, and bill length columns, filter for bill lengths greater than 40 mm, and then sort by bill length.\n\n# Combine select, filter, and arrange\npenguins_combined = penguins %&gt;%\n  select(species, island, bill_length_mm) %&gt;%\n  filter(bill_length_mm &gt; 40) %&gt;%\n  arrange(desc(bill_length_mm))\n\n# View the result\nhead(penguins_combined)\n\n# A tibble: 6 × 3\n  species   island bill_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;\n1 Gentoo    Biscoe           59.6\n2 Chinstrap Dream            58  \n3 Gentoo    Biscoe           55.9\n4 Chinstrap Dream            55.8\n5 Gentoo    Biscoe           55.1\n6 Gentoo    Biscoe           54.3\n\n\nThese dplyr functions provide a powerful and intuitive way to manipulate and analyze your data. With practice, you’ll find them indispensable for your data wrangling tasks."
  },
  {
    "objectID": "dplyr/dplyr.html#loading-the-necessary-libraries",
    "href": "dplyr/dplyr.html#loading-the-necessary-libraries",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "First, we’ll load the required libraries and the Palmer penguins dataset.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Take a look at the data\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "dplyr/dplyr.html#select-choose-specific-columns",
    "href": "dplyr/dplyr.html#select-choose-specific-columns",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "The select function allows us to choose specific columns from the dataset. For instance, we might only be interested in the species, island, and bill length columns.\n\n# Select specific columns\npenguins_selected = penguins %&gt;%\n  select(species, island, bill_length_mm)\n\n# View the first few rows of the selected columns\nhead(penguins_selected)\n\n# A tibble: 6 × 3\n  species island    bill_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;\n1 Adelie  Torgersen           39.1\n2 Adelie  Torgersen           39.5\n3 Adelie  Torgersen           40.3\n4 Adelie  Torgersen           NA  \n5 Adelie  Torgersen           36.7\n6 Adelie  Torgersen           39.3"
  },
  {
    "objectID": "dplyr/dplyr.html#filter-subset-rows-based-on-conditions",
    "href": "dplyr/dplyr.html#filter-subset-rows-based-on-conditions",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "The filter function helps us subset rows based on specific conditions. Let’s filter the data to only include observations where the bill length is greater than 40 mm.\n\n# Filter rows where bill length is greater than 40 mm\npenguins_filtered = penguins %&gt;%\n  filter(bill_length_mm &gt; 40)\n\n# View the first few rows of the filtered data\nhead(penguins_filtered)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           40.3          18                 195        3250\n2 Adelie  Torgersen           42            20.2               190        4250\n3 Adelie  Torgersen           41.1          17.6               182        3200\n4 Adelie  Torgersen           42.5          20.7               197        4500\n5 Adelie  Torgersen           46            21.5               194        4200\n6 Adelie  Biscoe              40.6          18.6               183        3550\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "dplyr/dplyr.html#mutate-create-or-transform-columns",
    "href": "dplyr/dplyr.html#mutate-create-or-transform-columns",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "With mutate, we can create new columns or transform existing ones. Suppose we want to calculate the bill length in centimeters.\n\n# Create a new column for bill length in cm\npenguins_mutated = penguins %&gt;%\n  mutate(bill_length_cm = bill_length_mm / 10)\n\n# View the first few rows to see the new column\nhead(penguins_mutated)\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_cm &lt;dbl&gt;"
  },
  {
    "objectID": "dplyr/dplyr.html#summarize-aggregate-data",
    "href": "dplyr/dplyr.html#summarize-aggregate-data",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "The summarize function allows us to compute summary statistics. To get a summary of the average bill length by species, we can use group_by in combination with summarize.\n\n# Summarize average bill length by species\npenguins_summary = penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(avg_bill_length = mean(bill_length_mm, na.rm = TRUE))\n\n# View the summary\npenguins_summary\n\n# A tibble: 3 × 2\n  species   avg_bill_length\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Adelie               38.8\n2 Chinstrap            48.8\n3 Gentoo               47.5"
  },
  {
    "objectID": "dplyr/dplyr.html#arrange-sort-data",
    "href": "dplyr/dplyr.html#arrange-sort-data",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "Finally, arrange allows us to sort the data. Let’s sort the penguins dataset by bill length in descending order.\n\n# Sort the data by bill length in descending order\npenguins_arranged = penguins %&gt;%\n  arrange(desc(bill_length_mm))\n\n# View the first few rows of the sorted data\nhead(penguins_arranged)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n4 Chinstrap Dream            55.8          19.8               207        4000\n5 Gentoo    Biscoe           55.1          16                 230        5850\n6 Gentoo    Biscoe           54.3          15.7               231        5650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "dplyr/dplyr.html#putting-it-all-together",
    "href": "dplyr/dplyr.html#putting-it-all-together",
    "title": "Wrangling Data with dplyr",
    "section": "",
    "text": "We can combine these verbs to perform more complex data manipulations. For example, let’s select the species, island, and bill length columns, filter for bill lengths greater than 40 mm, and then sort by bill length.\n\n# Combine select, filter, and arrange\npenguins_combined = penguins %&gt;%\n  select(species, island, bill_length_mm) %&gt;%\n  filter(bill_length_mm &gt; 40) %&gt;%\n  arrange(desc(bill_length_mm))\n\n# View the result\nhead(penguins_combined)\n\n# A tibble: 6 × 3\n  species   island bill_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;\n1 Gentoo    Biscoe           59.6\n2 Chinstrap Dream            58  \n3 Gentoo    Biscoe           55.9\n4 Chinstrap Dream            55.8\n5 Gentoo    Biscoe           55.1\n6 Gentoo    Biscoe           54.3\n\n\nThese dplyr functions provide a powerful and intuitive way to manipulate and analyze your data. With practice, you’ll find them indispensable for your data wrangling tasks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to BOI DS course tutorials",
    "section": "",
    "text": "This website contains tutorials and exercise sets on various topics to help you learn and apply concepts introduced in the course."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Welcome to BOI DS course tutorials",
    "section": "Topics",
    "text": "Topics\n\nData Wrangling with dplyr\n\nExercises for “Data Wrangling with dplyr”\n\nSimple linear regression\n\nExercises for “Simple linear regression”\n\nMultiple linear regression\n\nExercises for “Multiple linear regression”\n\nKNN regression\n\nExercises for “KNN regression”\n\nKNN classification\n\nExercises for “KNN classification”\n\nBayes classifier simulation\nTree regression\n\nExercises for “Tree regression”\n\nTree classification\n\nExercises for “Tree classification”\n\n[Cross validation]\n[Data sets]\n\nBoston dataset\nDefault dataset\n\n\nExplore the topics and start learning!"
  },
  {
    "objectID": "knn_classification/knn_classification_exercise_set.html",
    "href": "knn_classification/knn_classification_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "knn_classification/knn_classification_exercise_set.html#task-1-load-necessary-libraries",
    "href": "knn_classification/knn_classification_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "knn_classification/knn_classification_exercise_set.html#task-2-load-the-data",
    "href": "knn_classification/knn_classification_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "knn_classification/knn_classification_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "knn_classification/knn_classification_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "knn_classification/knn_classification_exercise_set.html#task-4-make-predictions",
    "href": "knn_classification/knn_classification_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "knn_regression/knn_regression_exercise_set.html",
    "href": "knn_regression/knn_regression_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "knn_regression/knn_regression_exercise_set.html#task-1-load-necessary-libraries",
    "href": "knn_regression/knn_regression_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "knn_regression/knn_regression_exercise_set.html#task-2-load-the-data",
    "href": "knn_regression/knn_regression_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "knn_regression/knn_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "knn_regression/knn_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "knn_regression/knn_regression_exercise_set.html#task-4-make-predictions",
    "href": "knn_regression/knn_regression_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "multiple_regression/multiple_regression_exercise_set.html",
    "href": "multiple_regression/multiple_regression_exercise_set.html",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "This exercise set is designed to help you practice the steps involved in performing K-Nearest Neighbors (KNN) regression using the Boston housing dataset. Each task is followed by a chunk of code that presents the solution.\n\n\nTask: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nTask: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()\n\n\n\n\n\nTask: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\n\n\nTask: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "multiple_regression/multiple_regression_exercise_set.html#task-1-load-necessary-libraries",
    "href": "multiple_regression/multiple_regression_exercise_set.html#task-1-load-necessary-libraries",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Load the tidyverse and tidymodels libraries.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "multiple_regression/multiple_regression_exercise_set.html#task-2-load-the-data",
    "href": "multiple_regression/multiple_regression_exercise_set.html#task-2-load-the-data",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Select the relevant columns from the Boston housing dataset and ensure there are no duplicate rows.\n\n\nShow the code\nboston_data = MASS::Boston %&gt;% \n  select(rm, medv) %&gt;% \n  distinct()"
  },
  {
    "objectID": "multiple_regression/multiple_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "href": "multiple_regression/multiple_regression_exercise_set.html#task-3-fit-the-knn-regression-model",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Fit a K-Nearest Neighbors (KNN) regression model to predict medv using rm. Set the number of neighbors to 100.\n\n\nShow the code\nknn_model_fit = nearest_neighbor(\n  mode = \"regression\",\n  neighbors = 100\n) %&gt;% \n  fit(medv ~ rm, data = boston_data)"
  },
  {
    "objectID": "multiple_regression/multiple_regression_exercise_set.html#task-4-make-predictions",
    "href": "multiple_regression/multiple_regression_exercise_set.html#task-4-make-predictions",
    "title": "KNN Regression with Boston Housing Data - Exercise Set",
    "section": "",
    "text": "Task: Use the fitted model to make predictions.\n\n\nShow the code\npredictions = knn_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "simple_regression/simple_regression.html",
    "href": "simple_regression/simple_regression.html",
    "title": "Simple Linear Regression with Boston Housing Data",
    "section": "",
    "text": "Introduction\nIn this tutorial, we’ll explore how to perform a simple linear regression using the Boston housing dataset. Linear regression is a foundational statistical method that allows us to model the relationship between a dependent variable and one or more independent variables. Here, we’ll investigate the relationship between the median value of owner-occupied homes (medv) and the average number of rooms per dwelling (rm).\nWe’ll be using the tidyverse and tidymodels libraries in R for data manipulation and modeling. Let’s get started!\n\n\nLoad Necessary Libraries\nFirst, we’ll load the required libraries. The tidyverse package provides tools for data manipulation and visualization, while tidymodels is a suite of packages for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\nLoad the Data\nNext, we’ll load the Boston housing dataset and select the relevant columns.\n\nboston_data = MASS::Boston %&gt;%\n  select(rm, medv)\n\n\n\nFit the Linear Regression Model\nNow, we’ll fit a simple linear regression model. Our goal is to predict medv using rm.\n\nlinear_model_fit = linear_reg() %&gt;% \n  fit(medv ~ rm, data = boston_data)\n\n\n\nInspect the Model Coefficients\nAfter fitting the model, it’s essential to inspect the coefficients to understand the relationship between the variables.\n\nlinear_model_fit %&gt;% \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -34.7      2.65      -13.1 6.95e-34\n2 rm              9.10     0.419      21.7 2.49e-74\n\n\n\n\nMake Predictions\nFinally, we’ll use our model to make predictions.\n\npredictions = linear_model_fit %&gt;% \n  predict(boston_data)\n\n\n\nConclusion\nIn this tutorial, we demonstrated how to perform a simple linear regression using the Boston housing dataset. We went through loading the data, fitting a model, inspecting the coefficients, and making predictions. This foundational technique can be applied to various datasets and provides a basis for more complex modeling.\nFeel free to explore further by adding more variables or trying different types of models. Happy coding!"
  },
  {
    "objectID": "tree_classification/tree_classification.html",
    "href": "tree_classification/tree_classification.html",
    "title": "Introduction to Tree Classification with R",
    "section": "",
    "text": "Welcome to this tutorial on tree classification. This tutorial aims to introduce the concept of tree classification, provide a detailed explanation of the code involved, and demonstrate its implementation using R. Tree classification is a powerful tool for predicting categorical outcomes based on input features, and it is widely used in various fields."
  },
  {
    "objectID": "tree_classification/tree_classification.html#what-is-tree-classification",
    "href": "tree_classification/tree_classification.html#what-is-tree-classification",
    "title": "Introduction to Tree Classification with R",
    "section": "What is Tree Classification?",
    "text": "What is Tree Classification?\nTree classification is a type of decision tree that is used for predicting categorical class labels. The tree is constructed by recursively splitting the data into subsets based on the values of input features, aiming to maximize the homogeneity of the target variable within each subset."
  },
  {
    "objectID": "tree_classification/tree_classification.html#loading-necessary-libraries",
    "href": "tree_classification/tree_classification.html#loading-necessary-libraries",
    "title": "Introduction to Tree Classification with R",
    "section": "Loading Necessary Libraries",
    "text": "Loading Necessary Libraries\nFirst, we need to load the required libraries for our analysis. We will use tidyverse for data manipulation and visualization, tidymodels for model fitting, and palmerpenguins for accessing the dataset.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "tree_classification/tree_classification.html#loading-the-data",
    "href": "tree_classification/tree_classification.html#loading-the-data",
    "title": "Introduction to Tree Classification with R",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe load and prepare the penguin_data dataset, filtering out the Chinstrap species and converting the species column to a factor.\n\npenguin_data = penguins %&gt;% \n  filter(!species == \"Chinstrap\") %&gt;% \n  mutate(species = factor(species))"
  },
  {
    "objectID": "tree_classification/tree_classification.html#visualizing-the-data",
    "href": "tree_classification/tree_classification.html#visualizing-the-data",
    "title": "Introduction to Tree Classification with R",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nWe start by visualizing the relationship between the predictor variables body_mass_g and bill_depth_mm from the penguin_data dataset.\n\npenguin_data %&gt;% \n  ggplot(aes(body_mass_g, bill_depth_mm, color = species)) + \n  geom_point()"
  },
  {
    "objectID": "tree_classification/tree_classification.html#fitting-the-tree-classification-model",
    "href": "tree_classification/tree_classification.html#fitting-the-tree-classification-model",
    "title": "Introduction to Tree Classification with R",
    "section": "Fitting the Tree Classification Model",
    "text": "Fitting the Tree Classification Model\nNext, we fit a tree classification model to predict species using bill_depth_mm and body_mass_g as predictors. We specify the model parameters, including the tree depth, minimum number of observations in a node, and the engine to be used.\n\ntree_model_fit = decision_tree(\n  cost_complexity = 0,\n  tree_depth = 1,\n  mode = \"classification\",\n  engine = \"rpart\",\n  min_n = 5\n) %&gt;%\n  fit(species ~ bill_depth_mm + body_mass_g, data = penguin_data)"
  },
  {
    "objectID": "tree_classification/tree_classification.html#making-predictions",
    "href": "tree_classification/tree_classification.html#making-predictions",
    "title": "Introduction to Tree Classification with R",
    "section": "Making Predictions",
    "text": "Making Predictions\nOnce the model is fitted, we use it to make predictions on the penguin_data.\n\npredictions = tree_model_fit %&gt;% \n  predict(penguin_data)"
  },
  {
    "objectID": "tree_classification/tree_classification.html#visualizing-the-tree",
    "href": "tree_classification/tree_classification.html#visualizing-the-tree",
    "title": "Introduction to Tree Classification with R",
    "section": "Visualizing the Tree",
    "text": "Visualizing the Tree\nFinally, we visualize the structure of the fitted tree to understand how the splits are made and how the predictions are generated.\n\ntree_model_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot(roundint = FALSE)"
  },
  {
    "objectID": "tree_classification/tree_classification.html#conclusion",
    "href": "tree_classification/tree_classification.html#conclusion",
    "title": "Introduction to Tree Classification with R",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial has provided an in-depth look at tree classification, from concept to implementation in R. By visualizing the data, fitting the model, making predictions, and visualizing the tree structure, we have demonstrated the entire process of tree classification analysis. Understanding these methods and their applications will enhance your ability to analyze and interpret categorical data effectively."
  },
  {
    "objectID": "tree_regression/tree_regression.html",
    "href": "tree_regression/tree_regression.html",
    "title": "Tree Regression with Boston Housing Data",
    "section": "",
    "text": "Welcome to this comprehensive tutorial on tree regression. This tutorial aims to introduce the concept of tree regression, provide a detailed explanation of the code involved, and demonstrate its implementation using the Boston housing dataset. Tree regression is a powerful tool for predicting continuous outcomes based on input features, and it is widely used in various fields."
  },
  {
    "objectID": "tree_regression/tree_regression.html#what-is-tree-regression",
    "href": "tree_regression/tree_regression.html#what-is-tree-regression",
    "title": "Tree Regression with Boston Housing Data",
    "section": "What is Tree Regression?",
    "text": "What is Tree Regression?\nTree regression is a type of decision tree that is used for predicting continuous values. Unlike classification trees that predict discrete class labels, regression trees predict a continuous target variable. The tree is constructed by recursively splitting the data into subsets based on the values of input features, aiming to minimize the variance within each subset."
  },
  {
    "objectID": "tree_regression/tree_regression.html#loading-necessary-libraries",
    "href": "tree_regression/tree_regression.html#loading-necessary-libraries",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Loading Necessary Libraries",
    "text": "Loading Necessary Libraries\nFirst, we need to load the required libraries for our analysis. We will use tidyverse for data manipulation and visualization and tidymodels for model fitting.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "tree_regression/tree_regression.html#load-the-data",
    "href": "tree_regression/tree_regression.html#load-the-data",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Load the Data",
    "text": "Load the Data\nNext, we’ll select the relevant columns from the Boston housing dataset.\n\nboston_data = MASS::Boston %&gt;%\n  select(medv, lstat)"
  },
  {
    "objectID": "tree_regression/tree_regression.html#visualizing-the-data",
    "href": "tree_regression/tree_regression.html#visualizing-the-data",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nWe start by visualizing the relationship between the predictor variable lstat and the target variable medv from the boston_data dataset.\n\nboston_data %&gt;% \n  ggplot(aes(lstat, medv)) + \n  geom_point()"
  },
  {
    "objectID": "tree_regression/tree_regression.html#fitting-the-tree-regression-model",
    "href": "tree_regression/tree_regression.html#fitting-the-tree-regression-model",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Fitting the Tree Regression Model",
    "text": "Fitting the Tree Regression Model\nNext, we fit a tree regression model to predict medv using lstat as the predictor. We specify the model parameters, including the tree depth, minimum number of observations in a node, and the engine to be used.\n\ntree_model_fit = decision_tree(\n  cost_complexity = 0,\n  tree_depth = 2,\n  mode = \"regression\",\n  engine = \"rpart\",\n  min_n = 5\n) %&gt;%\n  fit(medv ~ lstat, data = boston_data)"
  },
  {
    "objectID": "tree_regression/tree_regression.html#making-predictions",
    "href": "tree_regression/tree_regression.html#making-predictions",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Making Predictions",
    "text": "Making Predictions\nOnce the model is fitted, we use it to make predictions on the boston_data.\n\npredictions = tree_model_fit %&gt;% \n  predict(boston_data)"
  },
  {
    "objectID": "tree_regression/tree_regression.html#visualizing-the-tree",
    "href": "tree_regression/tree_regression.html#visualizing-the-tree",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Visualizing the Tree",
    "text": "Visualizing the Tree\nFinally, we visualize the structure of the fitted tree to understand how the splits are made and how the predictions are generated.\n\ntree_model_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot(roundint = FALSE)"
  },
  {
    "objectID": "tree_regression/tree_regression.html#conclusion",
    "href": "tree_regression/tree_regression.html#conclusion",
    "title": "Tree Regression with Boston Housing Data",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial has provided an in-depth look at tree regression, from concept to implementation in R. By visualizing the data, fitting the model, making predictions, and visualizing the tree structure, we have demonstrated the entire process of tree regression analysis. Understanding these methods and their applications will enhance your ability to analyze and interpret continuous data effectively."
  }
]